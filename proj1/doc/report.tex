\documentclass[11pt]{report}

\usepackage{minted}

\usepackage[utf8]{inputenc}
\usepackage{indentfirst}
\usepackage{titlesec}

\title{
    \textbf{Distributed Systems - Project 1} \\
    \Large T5G02
}

\author{
    \Large
    Eduardo Correia \\
    \texttt{up201806433@fe.up.pt}
    \and
    \Large
    Ana InÃªs Barros \\
    \texttt{up201806593@fe.up.pt}
}

\date{\today}

\begin{document}

\maketitle

\chapter{Introduction}

This report was elaborated for the \textit{Distributed Systems} (SDIS).

It serves as a complement for the course unit's first project and aims to describe the specification of the enhancements made to the several sub-protocols and how the concurrent execution of sub-protocols was achieved.

\chapter{Enhancements}

\section{Backup sub-protocol}

WIP ----> This scheme can deplete the backup space rather rapidly, and cause too much activity on the nodes once that space is full. Can you think of an alternative scheme that ensures the desired replication degree, avoids these problems, and, nevertheless, can inter-operate with peers that execute the chunk backup protocol described above?

\newpage

\section{Restore sub-protocol}

If the files we're restoring are too large, this protocol may not be desirable, for the following reasons:

Only one peer needs to receive the chunks that correspond to the file that's being restored, however, we are using a multi-cast channel to send them all, so even peers that didn't request the restore of the that file are receiving its chunks.

To eliminate this problem, we opted to use TCP, since it provides a direct and safe connection and it was required to obtain full credit.

Initiator Peer:

\begin{minted}{java}
socket = new ServerSocket(0);
socket.setSoTimeout(1000);

// Send Getchunk Message
message = new GetChunkEnhancedMsg(version, initiator_peer.id, file_id, 0, socket.getLocalPort());
control_channel.send(message.getBytes(null, 0));

// Open socket
Socket clientSocket = socket.accept();
DataInputStream inputStream = new DataInputStream(clientSocket.getInputStream());

// Receive message
int msg_len = inputStream.read(msg);

// Close connection
clientSocket.close();
socket.close();
\end{minted}

Peer who has chunk:

\begin{minted}{java}
// Open connection
socket = new Socket("localhost", get_chunk_msg.getPort());

// send msg
DataOutputStream outputStream = new DataOutputStream(socket.getOutputStream());
outputStream.write(message_bytes);
outputStream.flush();
socket.close()
\end{minted}

For this enhancement, the initiator peer opens a TCP server to receive data directly from the peer instead of the chunk being sent to the multicast channel. Port 0 is passed to the constructor of the server socket so that the socket uses an available port. 
We had to create a different GETCHUNK message for the enhanced version of this protocol where we add the port the peer needs to use in the constructor of the socket in order to open the connection with the initiator peer. If the connection is established, the peer sends the Chunk message to the intiator peer using a DataOutputStream and the connections is closed.

* Snippets of the code are simplified in order to focus on the TCP/IP connection

\newpage

\section{Delete sub-protocol}

If a peer that backs up some chunks of the file is not running at the time the initiator peer sends a DELETE message for that file, the space used by these chunks will never be reclaimed.

Code snippet:*

\begin{minted}{java} 
// Send WOKEUP message
WokeUpMsg woke_msg = new WokeUpMsg(peer.id);
peer.control_channel.send(woke_msg.getBytes(null, 0));

// Set of delete file's ids
private Set<String> deleted_files;
deleted_files = new ConcurrentHashMap<String, String>().newKeySet();

// Add file_id to set when asked to delete
deleted_files.add(file.getId());

// Upon receiving WOKEUP msg start delete protocol for each set entry
for(String file_id: deleted_files)
    pool.execute(new Delete(peer, version, file_id, mc_channel));
\end{minted} 

We created a new message: WOKEUP message. This message is sent when a peer starts running and is sent to the MC channel. Its purpose is to signal the beginning of execution of a peer.
With the enhanced delete protocol, the peer stores the file ids of file it has asked to delete. When a wakeup message is received, the WokeUpMessageHandler procceeds to start the delete sub protocol for each of the file ids of files that a peer asked to delete. By doing this, if a peer did not received the delete messages while it was not running, then it will receive when it wakes up. Furthermore, if a peer asks to backup a previously delete file again, the peer wont send more delete messages regarding that file when it receives a WOKEUP message.

*The code is simplified in order to focus on the enhancement.

\chapter{Concurrency Design}

Regarding concurrency, we opted for following most of the hints purposed in https://www.overleaf.com/project/6071d537fc84b5c0390baa17 .

THREADS

For each multicast channel there is a thread running which is responsible for receiving packets.  

\begin{minted}{java}
    @Override
    public void run() {
        if (start() != 0)
            return;

        running = true;

        while (running) {
            try {
                DatagramPacket packet = new DatagramPacket(buf, buf.length);
                socket.receive(packet); // Receive packet
                parseMessage(packet.getData(), packet.getLength());
            }

            catch (IOException e) {
                System.err.println("No more messages.");
                stop();
            }
        }
    }
\end{minted}

For the processing of the messages received, we created message handlers for each message type and for each of these handlers there is a thread. This makes it possible to process different messages at the same time. Each channel has its own CachedThreadPool that will execute each handler for each message. Example is bellow.

\begin{minted}{java}

public class MDR_Channel extends Channel {

    //etc ...

    @Override
    protected void parseMessage(byte[] msg, int msg_len) {
        byte[] header = Message.getHeaderBytes(msg);
        String[] header_fields = Message.getHeaderFields(msg);

        int sender_id = Integer.parseInt(header_fields[Fields.SENDER_ID.ordinal()]);
        String type = header_fields[Fields.MSG_TYPE.ordinal()];

        // Ignore message from itself
        if (sender_id == peer.id) return;

        if (type.equals("CHUNK")) {
            ChunkMessage chunk_msg = new ChunkMessage(header_fields);
            byte[] body = Message.getBodyBytes(msg, msg_len, header.length);
            
            // Log
            System.out.printf("< Peer %d received: %s\n", peer.id, chunk_msg.toString());
            
            // Chunk message handler
            pool.execute(new ChunkMessageHandler(chunk_msg, peer, body));
        }
    }
}

public class DeleteMessageHandler extends MessageHandler{
    private final Peer peer;

    public DeleteMessageHandler(DeleteMessage delete_msg, Peer peer){
        super(delete_msg.getFile_id(), peer.storage);
        this.peer = peer;
    }

    @Override
    public void run() {
        // Deletes all chunks from file
        deleteAllChunksFromFile(file_id);
        peer.saveStorage(); // Update storage
    }
    // etc
}
\end{minted}

In order to make it possible for a peer to execute multiple subprotocols at the same time (backup, restore, etc...), each subprotocol runs on a thread and peer has its own CachedThreadPool. This pool is responsible for executing the multicast channels threads and the subprotocol threads. Example bellow.

\begin{minted}{java}
public class Peer implements RMI {
public ExecutorService pool;

    public Peer(){
        pool = Executors.newCachedThreadPool();
        
        // Start listening on channels
        peer.pool.execute(peer.backup_channel);
        peer.pool.execute(peer.control_channel);
        peer.pool.execute(peer.restore_channel);
    }
    

    @Override
    public void restoreFile(String file_path) {
        BackedUpFile file = storage.getFileInfo(file_path);

        if (file == null) {
            System.out.println("File to restore needs to be backed up first. Aborting...");
            return;
        }

        Runnable task;
        if(version.equals("2.0"))
            task = new RestoreEnhanced(this, version, file.getPath(), file.getId(), file.getNumberOfChunks(), restore_channel, control_channel);
        else
            task = new Restore(this, version, file.getPath(), file.getId(), file.getNumberOfChunks(), restore_channel, control_channel);
        pool.execute(task);
    }
}
\end{minted}

Conclusion: 
Although threads allow us to process many messages and to run many subprotocols at the same time, thread objects use a significant amount of memory. Additionaly, in a large-scale application,allocating and deallocating many thread objects creates a significant memory management overhead. By using worker threads (thread pools) we minimized the overhead due to thread creation. We opted for using a CachedThreadPool because it creates new threads as needed, but will reuse previously constructed threads when they are available.

APPROPRIATE DATA STRUCTURES

With the use of threads, a need for appropriate data structures and thread-safe and synchronized methods. By using the java.util.concurrent package we were able to achieve concurrency in our program. 

We used a concurrent hash map(https://docs.oracle.com/javase/7/docs/api/java/util/concurrent/ConcurrentHashMap.html) which supports full concurrency of retrievals and adjustable expected concurrency for updates. 
- synchronized methods
- concurrent hash map (appropriate data structures)
- atomic variables


NO BLOCKING
- No sleeping
To avoid sleeping, you can use the class java.util.concurrent.ScheduledThreadPoolExecutor, which allows you to schedule a "timeout" handler, without using any thread before the timeout expires.
- asynchronous file channels

https://web.fe.up.pt/~pfs/aulas/sd2021/projs/proj1/concurrency\_hints.html


\end{document}
